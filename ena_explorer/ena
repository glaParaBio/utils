#!/usr/bin/env python3

import subprocess
import pandas
import argparse
import requests
import io
import sys
import re
import os

query_help = '''\
Return the metadata table for accession IDs or\nreturn the table of all available metadata
'''

query_epilog = '''\
---------------------------- EXAMPLES ---------------------------------------

Markdown table of all the metadata columns provided by ENA

    ena query -f markdown 

All metadata for accession(s)

    ena query PRJNA433164

Select some columns:

    ena query -f markdown -i 'sample_title|run_accession|fastq_ftp' PRJNA433164
'''

download_help = '''\
Download fastq files for accession IDs
'''

download_epilog = '''\
---------------------------- EXAMPLES ---------------------------------------

Use -n/--dryrun mode to only print the download commands

    ena download -n PRJNA433164

Make output filename more descriptive by prefixing it with sample_title:

    ena download -n -p '{sample_title}.' PRJNA433164

Only download the first n lines

    ena download -n -l 40000 PRJNA433164
'''

parser = argparse.ArgumentParser(description= 'Query for accessions and download fastq files from ENA', formatter_class= argparse.RawTextHelpFormatter)
parser.add_argument('--version', '-v', action='version', version='%(prog)s 0.1.0')

subparsers = parser.add_subparsers(help='', dest= 'subcommand')
subparsers.required= True

parser_query = subparsers.add_parser('query', help= query_help, description= query_help, formatter_class= argparse.RawDescriptionHelpFormatter)
parser_download = subparsers.add_parser('download', help= download_help, description= download_help, formatter_class= argparse.RawDescriptionHelpFormatter)

prefix_help = {'help': "Add this prefix to filenames. Use curly braces to add the value from the metadata table. Use 'AUTO' for automatically building a nice prefix. E.g., '{sample_title}.{library_strategy}.' [%(default)s]",
        'default': '', 'metavar': '[PREFIX|AUTO]'}

accession_file_help = {'help': "Read accessions from this file, one accession per line. It can be a tabular file where the first column are accessions. Use - to read from stdin", 'default': None}

parser_query.add_argument('accessions', help= 'List of accession IDs. With no accessions, return the table of metadata columns', nargs= '*')
parser_query.add_argument('--accession-file', '-a', **accession_file_help)
parser_query.add_argument('--prefix', '-p', **prefix_help)
parser_query.add_argument('--format', '-f', help= 'Output format [%(default)s]', choices= ['markdown', 'tsv'], default= 'tsv')
parser_query.add_argument('--transpose', '-t', help= 'Transpose table (more readable with more columns than rows) and use the given column as header e.g. run_accession', default= None)
parser_query.add_argument('--exclude-columns', '-e', help= 'Regular expression of columns to exclude [%(default)s]', default= '')
parser_query.add_argument('--include-columns', '-i', help= 'Regular expression of columns to include [%(default)s]', default= '.*')
parser_query.add_argument('--drop-invariant-cols', '-D', help= 'Drop columns with the same value across all rows', action= 'store_true')

parser_download.add_argument('accessions', help= 'List of accession IDs', nargs= '*')
parser_download.add_argument('--accession-file', '-a', **accession_file_help)
parser_download.add_argument('--prefix', '-p', **prefix_help)
parser_download.add_argument('--outdir', '-d', help= 'Change to this output directory before downloading [%(default)s]', default= '.')
parser_download.add_argument('--n-lines', '-l', help= 'Download up to this many lines per file, no limit if <= 0 [%(default)s]', default= -1, type= int)
parser_download.add_argument('--dryrun', '--dry-run', '-n', help= 'Only print the download commands', action= 'store_true')

class DownloadException(Exception):
    pass

class FormatException(Exception):
    pass

def get_available_fields(for_type= 'read_run', include= '.*', exclude= ''):
    url = "https://www.ebi.ac.uk/ena/portal/api/returnFields?dataPortal=ena&result={for_type}".format(for_type= for_type)
    r = requests.get(url)
    if r.status_code != 200:
        raise Exception('Return code %s' % r.status_code)
    txt = io.StringIO(r.text)
    table = pandas.read_csv(txt, sep='\t')
    selected = select_by_regex(table.columnId, include, exclude)
    table = table[table['columnId'].isin(selected)]
    table = table.set_index('columnId', drop= False)
    table = table.loc[selected]
    return(table)

def select_by_regex(lst, include= '.*', exclude= None):
    
    # This is will break if your regex has the pipe escaped (`\|`). We assume
    # this doesn't happen since column names don't contain | anyway.
    assert r'\|' not in include
    include = include.split('|')
    keep = []
    for regex in include:
        for x in lst:
            if re.search(regex, x) is not None and x not in keep:
                keep.append(x)
    
    if exclude is None or exclude == '':
        exclude= '$^'
    remove = []
    for x in keep:
        if re.search(exclude, x):
            remove.append(x)
    for x in remove:
        keep.remove(x)
    return keep

def get_table(accessions, fields, drop_empty, drop_invariant_cols, prefix= ''):
    if type(accessions) == str:
        accessions = [accessions]
    accessions = set(accessions)
    
    fields = list(fields)
    
    if len(accessions) == 0:
        return []
    
    results = [pandas.DataFrame(columns= fields)]
    accessions_not_found = []
    for a in accessions:
        url = "https://www.ebi.ac.uk/ena/portal/api/filereport?accession={accession}&fields={fields}&result=read_run".format(accession= a, fields= ','.join(fields))
        r = requests.get(url)
        if r.status_code == 200:
            txt = io.StringIO(r.text)
            table = pandas.read_csv(txt, sep='\t', usecols= fields)
            results.append(table)
        else:
            accessions_not_found.append(a)
            sys.stderr.write('Unable to find accession "%s" (ENA portal returned with code: %s)\n' % (a, r.status_code))
            # raise Exception('Return code %s' % r.status_code)
    results = pandas.concat(results)
    assert 'prefix' not in results.columns
    prefix_column = make_prefix(results, prefix)
    results.insert(0, 'prefix', prefix_column)

    if drop_empty:
        for x in results.columns:
            is_empty = all(pandas.isna(results[x])) or set(results[x]) == {''}
            if is_empty:
                results.drop(x, axis= 1, inplace= True)
    if drop_invariant_cols:
        for x in results.columns:
            is_invariant = len(set(results[x])) == 1
            if is_invariant:
                results.drop(x, axis= 1, inplace= True)

    results.drop_duplicates(inplace= True)
    query_results = {'results': results, 'accessions_not_found': accessions_not_found}
    return(query_results)

def get_column(table, column):
    if column in table.columns and not table[column].isnull().all():
        x = list(table[column])
    else:
        x = None
    return x

def nice_prefix(table, name_candidates= ['library_name', 'sample_title', 'sample_alias'], 
        extra= ['scientific_name', 'library_strategy'], sep= '.'):
    """Use the metadata table to generate nice prefix to add to output
    filenames
    """
    name = None
    for x in name_candidates:
        if get_column(table, x) is not None:
            name = x
            break
    
    prefix = ''
    if name is not None:
        prefix += '{' + name + '}'
    for x in extra:
        if get_column(table, x) is not None:
            prefix += sep + '{' + x + '}'
    prefix = prefix.lstrip(sep)
    if prefix != '':
        prefix += sep
    return prefix

def make_prefix(table, prefix):
    if prefix == 'auto':
        prefix = nice_prefix(table)
        sys.stderr.write('Using prefix "%s"\n' % prefix)
    prefix_fmt = []
    for idx,row in table.iterrows():
        fmt = prefix
        do_check = True
        for colname in table.columns:
            if '{' + colname + '}' in prefix:
                value = row[colname]
                if pandas.isnull(value):
                    value = 'NA'
                    sys.stderr.write('Warning: ENA returns NA for some fields in prefix "%s"\n' % prefix)
                else:
                    value = str(row[colname])
                if '{' in value and '}' in value:
                    do_check = False
                fmt = fmt.replace('{' + colname + '}', value)
        if do_check and '{' in fmt and '}' in fmt:
            raise FormatException('Some fields could not be formatted for %s' % prefix)
        fmt = re.sub('\s', '_', fmt)
        fmt = fmt.replace('/', '_')
        fmt = fmt.replace('\\', '_')
        fmt = fmt.replace("'", '"')
        if fmt.startswith('.'):
            fmt = '_' + fmt
        prefix_fmt.append(fmt)
    return prefix_fmt

def make_download_table(accessions, prefix):
    fields = get_available_fields().columnId
    query_results = get_table(accessions, fields, prefix=prefix, drop_empty= False, drop_invariant_cols= False)
    ftp = []
    dest_file = []
    fastq_bytes = []
    for idx,row in query_results['results'].iterrows():
        fastqs = row.fastq_ftp.split(';')
        fq_bytes = str(row.fastq_bytes).split(';')
        for x,b in zip(fastqs, fq_bytes):
            ftp.append(x)
            fastq_bytes.append(int(b))
            fn = row.prefix + os.path.basename(x)
            dest_file.append(fn)
    ftp = pandas.DataFrame({'fastq_ftp':ftp, 'dest_file': dest_file, 'fastq_bytes': fastq_bytes})
    assert len(list(ftp.dest_file)) == len(set(ftp.dest_file))
    return {'table': ftp, 'accessions_not_found': query_results['accessions_not_found']}

def transpose(table, header_column, metadata_column_name= 'metadata'):
    row_header = list(table.columns)
    col_header = None
    if header_column in table.columns:
        col_header = list(table[header_column])
    table = table.transpose()
    if col_header is not None:
        table.columns = col_header
    hdr = 'metadata'
    table.insert(0, 'metadata', row_header)
    return table


def download_cmd(cmd, dest_file, head):
    p = subprocess.Popen(cmd, shell=True, stdout= subprocess.PIPE, stderr= subprocess.PIPE, executable= 'bash')
    stdout, stderr = p.communicate()
    if (p.returncode != 0 and head == '') or (head != '' and p.returncode not in [0, 141]):
        try:
            os.remove(dest_file)
        except:
            pass    
        raise DownloadException('Exit code %s while executing:\n%s\n%s' %(p.returncode, cmd, stderr.decode()))
    return p


def download(url, dest_file, n_lines, dryrun, expected_bytes, attempt):
    if n_lines > 0:
        head = '| gzip -cd | head -n %s | gzip ' % n_lines
    else:
        head = ''
    if head == '':
        silent = ''
    else:
        silent = '-s '

    file_found = False
    if os.path.exists(dest_file) and os.path.getsize(dest_file) == expected_bytes and head == '':
        file_found = True
        sys.stderr.write('File %s found - download skipped\n' % dest_file)

    cmd = "curl {silent}-L {fastq_ftp} {head}> '{dest_file}'".format(silent= silent, head= head, fastq_ftp= url, dest_file= dest_file)
    if '|' in cmd:
        cmd = 'set -o pipefail && ' + cmd

    if dryrun:
        print(cmd)
    elif not file_found:
        sys.stderr.write(cmd + '\n')
        n_failed = 0
        while True:
            try:
                p = download_cmd(cmd, dest_file, head)
                break
            except DownloadException:
                n_failed += 1
                if n_failed < attempt:
                    sys.stderr.write('%s/%s Failed to execute %s\n' % (n_failed, attempt, cmd))
                else:
                    raise
    return cmd

def read_accession_file(accession_file):
    if accession_file is None or accession_file == '':
        return []
    if accession_file == '-':
        txt = sys.stdin.readlines()
    else:
        with open(accession_file) as fin:
            txt = fin.readlines()
    accessions = [re.sub(',.*|\t.*', '', x.strip()) for x in txt]
    accessions = [x for x in accessions if x != '']
    return accessions

if __name__ == '__main__':
    args = parser.parse_args()
    
    try:
        accessions = read_accession_file(args.accession_file)
    except:
        sys.stderr.write('Unable to read file %s\n' % args.accession_file)
        sys.exit(1)
    accessions = args.accessions + accessions

    if args.subcommand == 'query':
        for regex in [args.include_columns, args.exclude_columns]:
            try:
                re.compile(regex)
            except:
                sys.stderr.write('"%s" is not a valid regex\n' % regex)
                sys.exit(1)
        fields = get_available_fields(include= args.include_columns, exclude= args.exclude_columns)
        if len(fields) == 0:
            sys.stderr.write("There is no column selected with the provided regex's!\n")
            sys.exit(1)
        if len(accessions) == 0:
            query_results = {'results': fields, 'accessions_not_found': []}
        else:
            try:
                query_results = get_table(accessions= accessions, fields=
                        fields.columnId, prefix= args.prefix, drop_invariant_cols=
                        args.drop_invariant_cols, drop_empty= True)
            except FormatException:
                sys.stderr.write('Prefix "%s" cannot be formatted\n' % args.prefix)
                sys.exit(1)

        results = query_results['results']

        if args.transpose is not None:
            results = transpose(results, args.transpose)
        if args.format == 'markdown':
            try:
                print(results.to_markdown(index= False))
            except ImportError as e:
                if "Missing optional dependency 'tabulate'" in str(e):
                    sys.stderr.write("Failed to output in 'markdown' format because the python package 'tabulate' is not available.\nUse pip or conda to install tabulate\n")
                    sys.exit(1)
        elif args.format == 'tsv':
            csv = results.to_csv(None, sep= '\t', index= False)
            if not csv.endswith('\n'):
                csv += '\n'
            sys.stdout.write(csv)
        if len(query_results['accessions_not_found']) == 0:
            sys.exit(0)
        else:
            sys.exit(1)
    elif args.subcommand == 'download':
        if len(accessions) == 0:
            sys.stderr.write('download sub-command needs at least one accession\n')
            sys.exit(1)
        try:
            download_table = make_download_table(accessions, prefix= args.prefix)
        except FormatException:
            sys.stderr.write('Prefix "%s" cannot be formatted\n' % args.prefix)
            sys.exit(1)

        if not os.path.exists(args.outdir):
            sys.stderr.write('Creating output directory "%s"\n' % args.outdir)
            os.makedirs(args.outdir, exist_ok= True)
        os.chdir(args.outdir)
        
        for idx,row in download_table['table'].iterrows():
            cmd = download(row.fastq_ftp, row.dest_file, args.n_lines, args.dryrun, row.fastq_bytes, attempt= 1)
        if len(download_table['accessions_not_found']) == 0:
            sys.exit(0)
        else:
            sys.exit(1)
    sys.exit()

